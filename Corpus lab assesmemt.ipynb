{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "name": "CORPUS.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "PG4wBJIbquDr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#TASK 1 DEMOSTRATING COCA CORPUS"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yUXrtl9tquDy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "ee1e2597-19b6-4962-b557-c6578ea36dfc"
      },
      "source": [
        "#TASK 2 STEMMING:\n",
        "import nltk\n",
        "from nltk.stem import PorterStemmer\n",
        "Stemmerporter=PorterStemmer()\n",
        "Stemmerporter.stem('lemmatization')"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'lemmat'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TpbH6YKkquD5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "b3d7927f-0d55-4c2e-ab4d-de7e215d4dc4"
      },
      "source": [
        "#TASK 2 STEMMING:\n",
        "import nltk\n",
        "from nltk.stem import PorterStemmer\n",
        "Stemmerporter=PorterStemmer()\n",
        "Stemmerporter.stem('cheerfulness')"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'cheer'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jmB65KjfquD_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "f9b9b725-2893-42a5-f5e9-db838ea2bff4"
      },
      "source": [
        "import nltk\n",
        "from nltk.stem import LancasterStemmer\n",
        "stemmerLan =LancasterStemmer()\n",
        "stemmerLan.stem('happiness')\n"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'happy'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "sOnY_XCXquEF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "c2acbe9a-0168-4496-f340-19af92b4604e"
      },
      "source": [
        "import nltk\n",
        "from nltk.stem import RegexpStemmer\n",
        "stemmerregexp=RegexpStemmer('learn')\n",
        "stemmerregexp.stem('learning')\n"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'ing'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4XAu4phdquEJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "10a3e85f-df36-475d-bfcc-0d746ac7bc2b"
      },
      "source": [
        "import nltk\n",
        "from nltk.stem import SnowballStemmer\n",
        "SnowballStemmer.languages\n",
        "frenchstemmer=SnowballStemmer('french')\n",
        "frenchstemmer.stem('manges')"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'mang'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Krtt_qKquEM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "cadb53bf-59cb-4de9-f105-4480981fecdf"
      },
      "source": [
        "#TASK 3: STEMMING PARAGRAPHS\n",
        "\n",
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "stemmer = PorterStemmer()\n",
        "example = \"Am quick brown fox jumps over a lazy dog\"\n",
        "example = [stemmer.stem(token) for token in example.split(\" \")]\n",
        "print (\" \".join(example))\n"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Am quick brown fox jump over a lazi dog\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z7G9tgvjquEP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        },
        "outputId": "024c0d17-4ffd-49ee-b56f-b41d1cb48c35"
      },
      "source": [
        "# TASK 4: LEMMATIZER\n",
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "  \n",
        "import nltk\n",
        "from nltk.corpus import wordnet as wn\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "print(lemmatizer.lemmatize(\"cacti\"))\n",
        "print(lemmatizer.lemmatize(\"mice\"))\n",
        "print(lemmatizer.lemmatize(\"rocks\"))\n",
        "\n",
        "print(lemmatizer.lemmatize(\"better\", pos = 'a')) # given the part-of-speech, better lemmatizes to good\n",
        "\n",
        "print(lemmatizer.lemmatize(\"Am\"))   # This error is fixed when the PArt of Speech is given\n",
        "\n",
        "print(lemmatizer.lemmatize(\"am\", pos = 'v'))\n",
        "\n",
        "  "
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "cactus\n",
            "mouse\n",
            "rock\n",
            "good\n",
            "Am\n",
            "be\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AMkVh1JJquES",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "e0284359-062c-40fb-b28a-a73900751586"
      },
      "source": [
        "# TASK 5: CHINESE SEGMENTATION USING JIEBA\n",
        "\n",
        "import jieba\n",
        "seg = jieba.cut(\"把句子中所有的可以成词的词语都扫描出来\", cut_all = True)\n",
        "print(\" \".join(seg))"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "把 句子 中所 所有 的 可以 成 词 的 词语 都 扫描 描出 描出来 出来\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nZZnpeylquEU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 453
        },
        "outputId": "d8914956-b14b-4f27-cac6-788e62d6942f"
      },
      "source": [
        "# TASK 6: BASIC TEXT PROCESSING PIPELINE\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "import nltk\n",
        "sent = \"Become an expert in NLP\"\n",
        "words = nltk.word_tokenize(sent)\n",
        "print(words)\n",
        "\n",
        "texts = [\"\"\"The only true wisdom is in knowin' you know nothing.\n",
        "Beware the barrenness of a busy life.\n",
        "I decided that it was not wisdom that enabled poets to write their poetry,\n",
        "but a kind of ins. or inspiration, such as you find in seers and prophets who deliver all their sublime messages without knowing in the least what they mean. Be as you wish to seem. Wonder is the beginning of wisdom. Be kind, for everyone you meet is fighting a hard battle.  Our prayers should be for blessings in general, for God knows best what is good for us.\"\"\"]\n",
        "count=0;\n",
        "for text in texts:\n",
        "    sentences = nltk.sent_tokenize(text)\n",
        "    for sentence in sentences:\n",
        "        words = nltk.word_tokenize(sentence)\n",
        "        print(words)\n",
        "        tagged = nltk.pos_tag(words)\n",
        "        print(tagged)\n",
        "num_words = [len(sentence.split()) for sentence in texts]\n",
        "print('total word',num_words)"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "['Become', 'an', 'expert', 'in', 'NLP']\n",
            "['The', 'only', 'true', 'wisdom', 'is', 'in', 'knowin', \"'\", 'you', 'know', 'nothing', '.']\n",
            "[('The', 'DT'), ('only', 'JJ'), ('true', 'JJ'), ('wisdom', 'NN'), ('is', 'VBZ'), ('in', 'IN'), ('knowin', 'NN'), (\"'\", \"''\"), ('you', 'PRP'), ('know', 'VBP'), ('nothing', 'NN'), ('.', '.')]\n",
            "['Beware', 'the', 'barrenness', 'of', 'a', 'busy', 'life', '.']\n",
            "[('Beware', 'NNP'), ('the', 'DT'), ('barrenness', 'NN'), ('of', 'IN'), ('a', 'DT'), ('busy', 'JJ'), ('life', 'NN'), ('.', '.')]\n",
            "['I', 'decided', 'that', 'it', 'was', 'not', 'wisdom', 'that', 'enabled', 'poets', 'to', 'write', 'their', 'poetry', ',', 'but', 'a', 'kind', 'of', 'ins', '.']\n",
            "[('I', 'PRP'), ('decided', 'VBD'), ('that', 'IN'), ('it', 'PRP'), ('was', 'VBD'), ('not', 'RB'), ('wisdom', 'JJ'), ('that', 'IN'), ('enabled', 'VBD'), ('poets', 'NNS'), ('to', 'TO'), ('write', 'VB'), ('their', 'PRP$'), ('poetry', 'NN'), (',', ','), ('but', 'CC'), ('a', 'DT'), ('kind', 'NN'), ('of', 'IN'), ('ins', 'NNS'), ('.', '.')]\n",
            "['or', 'inspiration', ',', 'such', 'as', 'you', 'find', 'in', 'seers', 'and', 'prophets', 'who', 'deliver', 'all', 'their', 'sublime', 'messages', 'without', 'knowing', 'in', 'the', 'least', 'what', 'they', 'mean', '.']\n",
            "[('or', 'CC'), ('inspiration', 'NN'), (',', ','), ('such', 'JJ'), ('as', 'IN'), ('you', 'PRP'), ('find', 'VBP'), ('in', 'IN'), ('seers', 'NNS'), ('and', 'CC'), ('prophets', 'NNS'), ('who', 'WP'), ('deliver', 'VBP'), ('all', 'DT'), ('their', 'PRP$'), ('sublime', 'NN'), ('messages', 'NNS'), ('without', 'IN'), ('knowing', 'VBG'), ('in', 'IN'), ('the', 'DT'), ('least', 'JJS'), ('what', 'WP'), ('they', 'PRP'), ('mean', 'VBP'), ('.', '.')]\n",
            "['Be', 'as', 'you', 'wish', 'to', 'seem', '.']\n",
            "[('Be', 'VB'), ('as', 'IN'), ('you', 'PRP'), ('wish', 'VBP'), ('to', 'TO'), ('seem', 'VB'), ('.', '.')]\n",
            "['Wonder', 'is', 'the', 'beginning', 'of', 'wisdom', '.']\n",
            "[('Wonder', 'NNP'), ('is', 'VBZ'), ('the', 'DT'), ('beginning', 'NN'), ('of', 'IN'), ('wisdom', 'NN'), ('.', '.')]\n",
            "['Be', 'kind', ',', 'for', 'everyone', 'you', 'meet', 'is', 'fighting', 'a', 'hard', 'battle', '.']\n",
            "[('Be', 'NNP'), ('kind', 'NN'), (',', ','), ('for', 'IN'), ('everyone', 'NN'), ('you', 'PRP'), ('meet', 'VBP'), ('is', 'VBZ'), ('fighting', 'VBG'), ('a', 'DT'), ('hard', 'JJ'), ('battle', 'NN'), ('.', '.')]\n",
            "['Our', 'prayers', 'should', 'be', 'for', 'blessings', 'in', 'general', ',', 'for', 'God', 'knows', 'best', 'what', 'is', 'good', 'for', 'us', '.']\n",
            "[('Our', 'PRP$'), ('prayers', 'NNS'), ('should', 'MD'), ('be', 'VB'), ('for', 'IN'), ('blessings', 'NNS'), ('in', 'IN'), ('general', 'JJ'), (',', ','), ('for', 'IN'), ('God', 'NNP'), ('knows', 'VBZ'), ('best', 'JJS'), ('what', 'WP'), ('is', 'VBZ'), ('good', 'JJ'), ('for', 'IN'), ('us', 'PRP'), ('.', '.')]\n",
            "total word [100]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KXSq_Bu2quEY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}